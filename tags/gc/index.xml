<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gc on 7sharp9</title>
    <link>http://7sharpnine.com/tags/gc/</link>
    <description>Recent content in Gc on 7sharp9</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Dave Thomas</copyright>
    <lastBuildDate>Sun, 11 Dec 2011 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://7sharpnine.com/tags/gc/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Fixing a hole...</title>
      <link>http://7sharpnine.com/2011/12/11/2011-12-11-fixing-a-hole/</link>
      <pubDate>Sun, 11 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>http://7sharpnine.com/2011/12/11/2011-12-11-fixing-a-hole/</guid>
      <description>&lt;p&gt;Due to popular demand&amp;hellip; well, I had a couple of requests anyway :-) Heres
a post inspired by my recent encounters profiling some of the code in
&lt;a href=&#34;https://github.com/fractureio/fracture&#34;&gt;Fracture-IO&lt;/a&gt;.  &lt;!-- more --&gt;I have recently been
profiling the code in fracture to remove any so called low hanging fruits.
During this time I also noticed an increase in memory allocation.  I
remembered I had recently been experimenting in a branch using pipelets as a
buffer between the send and receive stages in the Http Server, so I set up a
simple test to see if pipelets were contributing to the memory allocation
issues I was seeing.  Here&amp;rsquo;s the simple iteration test code I used for the
memory profiling:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fsharp&#34;&gt;open System
open System.Diagnostics
open System.Threading
open Fracture.Pipelets  
let reverse (s:string) =
  String(s |&amp;gt; Seq.toArray |&amp;gt; Array.rev)  
let oneToSingleton a b f=
  let result = b |&amp;gt; f
  result |&amp;gt; Seq.singleton  
/// Total number to run through test cycle
let number = 100  
/// To Record when we are done
let counter = ref 0
let sw = new Stopwatch()
let countThis (a:String) =
  do Interlocked.Increment(counter) |&amp;gt; ignore
  if !counter % number = 0 then
    sw.Stop()
    printfn &amp;quot;Execution time: %A&amp;quot; sw.Elapsed.TotalMilliseconds
    printfn &amp;quot;Items input: %d&amp;quot; number
    printfn &amp;quot;Time per item: %A ms (Elapsed Time / Number of items)&amp;quot;
      (TimeSpan.FromTicks(sw.Elapsed.Ticks / int64 number).TotalMilliseconds)
    printfn &amp;quot;Press any key to repeat, press &#39;q&#39; to exit.&amp;quot;
    sw.Reset()
  counter |&amp;gt; Seq.singleton  
let OneToSeqRev a b =
  oneToSingleton a b reverse   
let generateCircularSeq (s) =
  let rec next () =
    seq {
      for element in s do
        yield element
      yield! next()
    }
  next()  
    let stage1 = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Stage1&amp;quot;, OneToSeqRev &amp;quot;1&amp;quot;, Routers.roundRobin, number, -1)
    let stage2 = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Stage2&amp;quot;, OneToSeqRev &amp;quot;2&amp;quot;, Routers.basicRouter, number, -1)
    let stage3 = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Stage3&amp;quot;, OneToSeqRev &amp;quot;3&amp;quot;, Routers.basicRouter, number, -1)
    let stage4 = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Stage4&amp;quot;, OneToSeqRev &amp;quot;4&amp;quot;, Routers.basicRouter, number, -1)
    let stage5 = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Stage5&amp;quot;, OneToSeqRev &amp;quot;5&amp;quot;, Routers.basicRouter, number, -1)
    let stage6 = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Stage6&amp;quot;, OneToSeqRev &amp;quot;6&amp;quot;, Routers.basicRouter, number, -1)
    let stage7 = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Stage7&amp;quot;, OneToSeqRev &amp;quot;7&amp;quot;, Routers.basicRouter, number, -1)
    let stage8 = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Stage8&amp;quot;, OneToSeqRev &amp;quot;8&amp;quot;, Routers.basicRouter, number, -1)
    let stage9 = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Stage9&amp;quot;, OneToSeqRev &amp;quot;9&amp;quot;, Routers.basicRouter, number, -1)
    let stage10 = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Stage10&amp;quot;, OneToSeqRev &amp;quot;10&amp;quot;, Routers.basicRouter, number, -1)
    let final = new Pipelet&amp;lt;_,_&amp;gt;(&amp;quot;Final&amp;quot;, countThis, Routers.basicRouter, number, -1)  
    let manyStages = [stage2;stage3;stage4;stage5;stage6;stage7;stage8;stage9;stage10]  
    oneToMany stage1 manyStages
    manyToOne manyStages final  
    System.AppDomain.CurrentDomain.UnhandledException |&amp;gt; Observable.add (fun x -&amp;gt;
      printfn &amp;quot;%A&amp;quot; (x.ExceptionObject :?&amp;gt; Exception); Console.ReadKey() |&amp;gt; ignore)  
    let circ = [&amp;quot;John&amp;quot;; &amp;quot;Paul&amp;quot;; &amp;quot;George&amp;quot;; &amp;quot;Ringo&amp;quot;; &amp;quot;Nord&amp;quot;; &amp;quot;Bert&amp;quot;] |&amp;gt; generateCircularSeq   
    let startoperations() =
      sw.Start()
      for str in circ |&amp;gt; Seq.take number
        do  str --&amp;gt; stage1
      printfn &amp;quot;Insert complete waiting for operation to complete.&amp;quot;  
    printfn &amp;quot;Press any key to process %i items&amp;quot; number
    while not (Console.ReadKey().Key = ConsoleKey.Q) do
      startoperations()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using process explorer from Mark Russinovich I watched the allocated memory
grow as the iterations progressed:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh4.googleusercontent.com/-VP1-Vo2VINU/TuS7yZFTTlI/AAAAAAAABNw/3ksn5vNXTtw/s400/leak.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;Theres definitely something leaking in there! So what can we do to find this?
Simple, we use a memory profiler.  There are several really good memory
profilers out there.  I have listed some of the best ones below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://memprofiler.com/&#34;&gt;SciTech memory profiler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.red-gate.com/products/dotnet-development/ants-memory-profiler/&#34;&gt;RedGates ANTS Memory Profiler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jetbrains.com/profiler/&#34;&gt;JetBrains dotTrace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.yourkit.com/dotnet/features/index.jsp&#34;&gt;YourKit Profiler for .NET&lt;/a&gt;
To demonstrate finding the leak I will be using &lt;a href=&#34;http://www.red-gate.com/products/dotnet-development/ants-memory-
profiler/&#34;&gt;RedGates ANTS MemoryProfiler&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First of all we launch the profiler and set it up to profile the application,
this is just a simple case of browsing to the release folder and picking the
application so I won&amp;rsquo;t bore with those trivial details here. Now that the
application is running we hit any key which caused the test application to
post 100 operations into the pipeline.  We want to create a baseline snapshot
of the memory allocation so we can see where our leak is.  To do this click
Take Memory Snapshot at the top right of the screen.  Next we hit any key
again in the test application, again causing it to post another 100 operations
into the pipeline.  Now we click Take Memory Snapshot again. Now we have a
snapshot of the difference between the two operations.  The summery screen is
shown below:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh4.googleusercontent.com/-N72POVbq0ZA/TuS7v0xBt1I/AAAAAAAABNQ/ExKfkJDCb50/s912/3%2Bsummary.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;From this screen you can see that there is 51.56KB of new memory allocated
since the last snapshot, and you can see some nice piecharts showing the
various allocations in G1, G2 etc.  On the right hand side of the pie chart
you can see that the largest classes are: object[], AsyncParamsAux,
Pipelets+loop@37-7&lt;Unit, string,string&gt;, and AsyncParams&lt;Unit&gt;.&lt;/p&gt;

&lt;p&gt;Now if we click on Class List button we can investigate these further, heres
the Class List:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh4.googleusercontent.com/-UDU20rPk7ck/TuS7xvK_67I/AAAAAAAABNg/tnxTlpBYBJg/s912/4%2Bclass%2Blist.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;Here things start to get interesting.  If you click on the instance Diff (+/-)
column you can sort the list of classed by the differences to the last
snapshot.&lt;/p&gt;

&lt;p&gt;Now looking at the results we have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;300 more instances of AsyncBuilderImpl, AsyncParamsArgs, and AsyncParams&lt;/li&gt;
&lt;li&gt;200 more instances of Pipelets+loop@37-7&lt;Unit, string, string&gt;&lt;/li&gt;
&lt;li&gt;100 more instances of Pipelets+loop@37-7&lt;Unit, string, FSharpRef&lt;int&gt;&amp;gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Is it a coincidence that we just pushed 100 operations through the pipeline?
I think not!&lt;/p&gt;

&lt;p&gt;Now that we have a target for further inspection we can highlight the row for
the function &lt;strong&gt;Pipelets+loop@37-7&lt;Unit, string, FSharpRef&lt;int&gt;&amp;gt;&amp;gt;&lt;/strong&gt; and then
click on the icon that has three little blue boxes on it.  This will take us
to the instance List as shown below:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh6.googleusercontent.com/-csF589rWobQ/TuS7wzT6NWI/AAAAAAAABNY/TM95SUmaCEQ/s912/5%2Binstance%2Blist.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;I have sorted the instance list by the distance from the GC Root, you can see
there is a strange pattern emerging, the GC root distant increase by three
each time.  Now lets look at the Instance Retention graph for the first one
with a GC Root distance of 9, this is the icon on the right hand side of the
function name, it looks like a few rectangles joined up with a line:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh3.googleusercontent.com/-9orgQ4etsdI/TuS7yE-Dh7I/AAAAAAAABNs/0-dc3x8u-Mo/s525/first.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;The Pipelets+loop function is linked from the mailbox processor shown at the
top of the graph and flows into the Async infrastructure, and finally to the
loop function at the bottom.&lt;/p&gt;

&lt;p&gt;Lets look at the next one, this has a GC Root distance of 12:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh3.googleusercontent.com/-j1eALRVz0kA/TuS7z0ycXDI/AAAAAAAABN8/SGPVpPQbSz0/s531/second.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;If you look carefully there is another pattern here, the field references
args, aux@, econt@ are repeated in the red boxes.  The functions look to be
quite similar too.  Lets look at the next one GC Root Distance  of 15:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh6.googleusercontent.com/-d3V1WkCktTM/TuS7zxVnizI/AAAAAAAABOA/wA5xlP-UTxA/s646/third.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;Looking at this we have a definite repeat of the functions and arguments,  if
we look down to GC Root at a depth of 60 we get this:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh3.googleusercontent.com/-X3v-_UfEkI4/TuS7xzETTHI/AAAAAAAABNk/js7xg3GTHIo/s640/60.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;So whats happening here is that there is a continuation that has been built
around the asynchronous calls that gets bigger and bigger on each iteration.&lt;/p&gt;

&lt;p&gt;Now that we have identified the leak, lets look at the code and see whats
going on.  That would be the loop function in Pipelets:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fsharp&#34;&gt;let mailbox = MailboxProcessor.Start(fun inbox -&amp;gt;
  let rec loop routes = async {
    let! msg = inbox.Receive()
    match msg with
    | Payload(data) -&amp;gt;
      ss.Release() |&amp;gt; ignore
      try
        data |&amp;gt; transform |&amp;gt; router &amp;lt;| routes
        return! loop routes
      with //force loop resume on error
      | ex -&amp;gt; errors ex
          return! loop routes
    | Attach(stage) -&amp;gt; return! loop (stage::routes)
    | Detach(stage) -&amp;gt; return! loop (List.filter (fun x -&amp;gt; x &amp;lt;&amp;gt; stage) routes)
  }
  loop [])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Have a look at lines 9 and 12.  Can you guess whats wrong?&lt;/p&gt;

&lt;p&gt;Well, to quote the &lt;a href=&#34;http://blogs.msdn.com/b/fsharpteam/archive/2011/07/08/tail-calls-in-fsharp.aspx&#34;&gt;F# Teams blog&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;On the .NET platform, there are limitations on where tail calls may occur.
One restriction is that tail calls cannot be performed in try-catch or try-
finally blocks (neither in the body of the try nor in the catch or finally
handlers).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It goes on further to discuss another subtle issue with use bindings:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;use bindings implicitly generate a try-finally around the code that follows
them to ensure that the Dispose method is called on the bound value.  This
means that no calls following a use binding will be tail calls.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So all we have to do change the way the try catch block is formulated in that
section.  The most idiomatic way of dealing with this is to use the
&lt;a href=&#34;http://msdn.microsoft.com/en-us/library/ee353899.aspx&#34;&gt;Async.Catch function&lt;/a&gt;
which would result in code something like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fsharp&#34;&gt;let mailbox = MailboxProcessor.Start(fun inbox -&amp;gt;
  let rec loop routes = async {
    let! msg = inbox.Receive()
    match msg with
    | Payload(data) -&amp;gt;
      ss.Release() |&amp;gt; ignore
      let result = async{data |&amp;gt; transform |&amp;gt; router &amp;lt;| routes} 
      |&amp;gt; Async.Catch 
      |&amp;gt; Async.RunSynchronously
      match result with
      | Choice1Of2() -&amp;gt; ()
      | Choice2Of2 exn -&amp;gt; errors exn
      return! loop routes
    | Attach(stage) -&amp;gt; return! loop (stage::routes)
    | Detach(stage) -&amp;gt; return! loop (List.filter (fun x -&amp;gt; x &amp;lt;&amp;gt; stage) routes)
  }
  loop [])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively you could move the entire try with section out to a more local
section thats not in the recursive async loop construct:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fsharp&#34;&gt;let computeAndRoute data routes =
  try
    data |&amp;gt; transform |&amp;gt; router &amp;lt;| routes
    Choice1Of2()
  with
  | ex -&amp;gt; Choice2Of2 ex  

let mailbox = MailboxProcessor.Start(fun inbox -&amp;gt;
  let rec loop routes = async {
    let! msg = inbox.Receive()
    match msg with
    | Payload(data) -&amp;gt;
      ss.Release() |&amp;gt; ignore
      match computeAndRoute data routes with
      | Choice2Of2 exn -&amp;gt; errors exn
      | _ -&amp;gt; ()
      return! loop routes
    | Attach(stage) -&amp;gt; return! loop (stage::routes)
    | Detach(stage) -&amp;gt; return! loop (List.filter (fun x -&amp;gt; x &amp;lt;&amp;gt; stage) routes)}
  loop [])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Anyway I hope that sheds a bit of light on how to spot where memory leaks are
stemming from, and also some of the little known and often forgotten caveats
with tail recursion.&lt;/p&gt;

&lt;p&gt;Until next time&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: Just to make things a little bit clearer.  The memory leak here is caused by the async block being transformed into chains of continuation passing-style functions, and due to tail call elimination not being possible inside of the try catch blocks, the continuation grows and grows during each recursion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sockets and Bockets 4</title>
      <link>http://7sharpnine.com/2011/01/28/2011-01-28-sockets-and-bockets-part-4/</link>
      <pubDate>Fri, 28 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>http://7sharpnine.com/2011/01/28/2011-01-28-sockets-and-bockets-part-4/</guid>
      <description>

&lt;h2 id=&#34;welcome-to-part-4&#34;&gt;Welcome to part 4&lt;/h2&gt;

&lt;p&gt;If you were looking forward to some exciting new F# code this time your going
to be disappointed, however if you are like me and like looking at graphs and
stats and digging in deeper into the code then your going to enjoy this, lets
get started&amp;hellip;&lt;!-- more --&gt;&lt;/p&gt;

&lt;p&gt;I set up a 5 minute test with 50 clients connecting to the server with a 15ms
interval between each one.  Once connected each client receives a 128 byte
message from the server every 100ms so this will be a 500 messages per second
test.  I am going to be using an excellent product called &lt;a href=&#34;http://bit.ly/e4ToaO&#34;&gt;YourKit Profilerfor .NET&lt;/a&gt; it can do both memory and CPU profiling as
well as displaying telemetry for things like thread count, stack contents,
memory allocations etc.  It can be configured to be a lot less intrusive than
a lot of other profilers and I have had a lot of success using it.  You can
download a demo from their site using the link above.  I will be doing some
other articles on using profiling and analysis tools later on so stay tuned
for those too.  All of the graphs and information gathered in this post come
from YourKits output during CPU and memory profiling.&lt;/p&gt;

&lt;p&gt;Before we start here&amp;rsquo;s a reminder of what the client code looks like, this is
a simple test client using Brian&amp;rsquo;s code as mentioned in
&lt;a href=&#34;http://7sharpnine.com/posts/sockets-and-bockets-1/&#34;&gt;Part1&lt;/a&gt; I have highlighted the lines that
have changed below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fsharp&#34;&gt;    open System.Net
    open System.Net.Sockets  
    let quoteSize = 128  
    type System.Net.Sockets.TcpClient with
      member client.AsyncConnect(server, port, clientIndex) =
        Async.FromBeginEnd(server, port,(client.BeginConnect : IPAddress * int * _ * _ -&amp;gt; _), client.EndConnect)  
    let clientRequestQuoteStream (clientIndex, server, port:int) =
      async {
        let client = new System.Net.Sockets.TcpClient()
        do!  client.AsyncConnect(server,port, clientIndex)
        let stream = client.GetStream()
        let! header = stream.AsyncRead 1 // read header
        while true do
          let! bytes = stream.AsyncRead quoteSize
          if Array.length bytes &amp;lt;&amp;gt; quoteSize then
            printfn &amp;quot;client incorrect checksum&amp;quot;
      }  
    let myLock = new obj()  
    let clientAsync clientIndex =
      async {
        do! Async.Sleep(clientIndex*15)
        if clientIndex % 10 = 0 then
          lock myLock (fun() -&amp;gt; printfn &amp;quot;%d clients...&amp;quot; clientIndex)
        try
          do! clientRequestQuoteStream (clientIndex, IPAddress.Loopback, 10003)
        with e -&amp;gt;
          printfn &amp;quot;CLIENT %d ERROR: %A&amp;quot; clientIndex e
          //raise e
      }  
    Async.Parallel [ for i in 1 .. 50 -&amp;gt; clientAsync i ]
      |&amp;gt; Async.Ignore
      |&amp;gt; Async.Start
    System.Console.ReadKey() |&amp;gt; ignore
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cpu-and-threading-performance&#34;&gt;CPU and threading performance&lt;/h2&gt;

&lt;p&gt;First of all lets look at the CPU results from the &lt;em&gt;IAsync&lt;/em&gt; pattern:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh5.googleusercontent.com/-3H8-TiiB-VI/TwYhL2mvYsI/AAAAAAAABQI/z8dmiHBvTJE/mcnamara-cpu1.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;Heres the same run from the SAEA pattern:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh3.googleusercontent.com/-yGB2zdE3kGM/TwYhLPrUmLI/AAAAAAAABP8/Y6CIMWOi4Gk/Bocket-cpu2.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;You can see that both the number of threads and the amount of CPU is a quite a
lot less in the SAEA pattern.  The spike at the beginning is the allocation of
buffers for the BocketPool.&lt;/p&gt;

&lt;p&gt;Now lets move on to memory and garbage collection.&lt;/p&gt;

&lt;h2 id=&#34;memory-allocation&#34;&gt;Memory Allocation&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s a graph of the heap and process memory allocation in the &lt;strong&gt;IAsync&lt;/strong&gt;
pattern, green is generation 0, blue is generation 1 and orange is the large
object heap.  There&amp;rsquo;s also red for generation 2 but the results are behind the
others and they are only small 0,2 MB peaks at 5 to 15 second intervals.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh3.googleusercontent.com/-PalohQxAkOg/TwYhL6hR5JI/AAAAAAAABQQ/NtCLYc43OZc/mcnamara-mem1.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;Heres the same but for the SAEA pattern, there are red peaks every 10- 20
second intervals of 0.2MB hidden under the others.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh3.googleusercontent.com/-Nadz1nXQ7lg/TwYhLBXMvcI/AAAAAAAABQM/xCfuXkzkekM/Bocket-mem1.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;As you can see the heap memory is around half the size and the process memory is 15MB less.&lt;/p&gt;

&lt;h2 id=&#34;memory-hotspots&#34;&gt;Memory Hotspots&lt;/h2&gt;

&lt;p&gt;Finally here&amp;rsquo;s a couple of screen shot of the hot spots for memory allocations
in both implementations&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lh4.googleusercontent.com/-n3QWLgvNjq8/TwYhLX8cIZI/AAAAAAAABQA/LkeRno775Ew/s800/IAsync-hot.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;IAsync&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;



&lt;figure &gt;
    
        &lt;img src=&#34;https://lh5.googleusercontent.com/-PSX_YUfxkgU/TwYhMr40DTI/AAAAAAAABQY/D8bgLS6kNwc/s800/SAEA-hot.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;SAEA&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;You can clearly the &lt;strong&gt;IAsync&lt;/strong&gt; allocations are not present in the SAEA
implementation and there are 310,188 of them, that&amp;rsquo;s 27% of the total garbage!&lt;/p&gt;

&lt;h2 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h2&gt;

&lt;p&gt;The SAEA pattern definitely cuts down on memory and CPU usage, yes it adds a
lot of complexity but if your application is dealing with a very high volume
of traffic or clients and you need optimal performance then I think its the
way to go.&lt;/p&gt;

&lt;p&gt;The optimisations don&amp;rsquo;t stop there either, if you think about it the receive
Bocketpool is not even used here, if we collapsed all of the BocketPools into
a single contiguous store then we would use even less resources, this means we
could support even more clients or throughput.  In a typical high volume
scenario you are looking at doubling your throughput or number of client
connections.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s definitely a lot more or interesting things to explore in this area.&lt;/p&gt;

&lt;p&gt;As usual any comments are welcome.&lt;/p&gt;

&lt;p&gt;See you next time&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sockets and Bockets 1</title>
      <link>http://7sharpnine.com/2011/01/13/2011-01-13-sockets-and-bockets-1/</link>
      <pubDate>Thu, 13 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>http://7sharpnine.com/2011/01/13/2011-01-13-sockets-and-bockets-1/</guid>
      <description>

&lt;h3 id=&#34;welcome-to-part-1&#34;&gt;Welcome to part 1&lt;/h3&gt;

&lt;p&gt;A while back I read an interesting article by &lt;em&gt;Brian McNamara&lt;/em&gt; &lt;a href=&#34;http://lorgonblog.wordpress.com/2010/03/28/f-async-on-the-server-side/&#34;&gt;f-async-on-the-server-side&lt;/a&gt;
which describes C# and F# versions of a simple asynchronous
socket server, one of the driving forces behind the article was how F# can
wrap the traditional asynchronous model with &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/dd233250.aspx&#34;&gt;Asynchronous Workflows&lt;/a&gt;, this
produces nice clean simple code compared to the C# version which uses lambda
expressions, the code looks quite ugly in this style!  However thats not the
end of the story, a lot of memory fragmentation can occur using the APM model
when there is a high throughput, so I thought I would see if I could take this
a step further&amp;hellip;&lt;!-- more --&gt;&lt;/p&gt;

&lt;p&gt;There are some lesser known methods that were added to the &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/system.net.sockets.socket.aspx&#34;&gt;Socket&lt;/a&gt;
class in .Net 2.0 SP1: ReceiveAsync, SendAsync, ConnectAsync and DisconnectAsync.
These methods use an event driven model and &lt;strong&gt;do not&lt;/strong&gt; result in the creation
of AsyncResult objects, these are created on every asynchronous call in the
traditional Socket Begin/End methods.  Once you have thousands of clients
sending and receiving thousands of messages all of the object creation can
really have an adverse effect on performance on the garbage collected, you
will regularly see the AsyncResult objects hitting Generation 1 and 2.&lt;/p&gt;

&lt;p&gt;To use the xxxAsync methods you have pass a &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/system.net.sockets.socketasynceventargs.aspx&#34;&gt;SocketAsyncEventArgs&lt;/a&gt;object which is
assigned callback method and a buffer, the callback method called
asynchronously when the operation completes and is passed the corresponding
SocketAsyncEventArgs object, this allows you query the buffer in a receive
operation.&lt;/p&gt;

&lt;p&gt;The scope of this series of articles is to initially replicate Brian&amp;rsquo;s demo
using F# and a pool of SocketAsyncEventArgs and a contiguous block of memory
to hold the data being sent and received on the Socket, this again further
reduces memory fragmentation on the send and receive buffers.&lt;/p&gt;

&lt;p&gt;I have successfully developed an enterprise server for a client using this
method, it processed thousands of simultaneous connected clients and messages,
key components in the system were the High performance sockets, a pipeline
processor and a highly efficiency means of data compaction, I will only be
including the High performance sockets in this series but the other components
will be at a later date in separate articles.  Interestingly all of the code
was originally developed in c# but had a distinctly functional style, even the
Pipeline Processing is reminiscent of functional composition using the F#
pipeline operator &lt;strong&gt;|&amp;gt;&lt;/strong&gt; although an analogue of attach and detach was used
which in itself is declarative.&lt;/p&gt;

&lt;p&gt;Although there is no code in this article there is plenty in the next!&lt;/p&gt;

&lt;p&gt;Please feel free to leave comments or add any suggestions, hope to see you
next time&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>